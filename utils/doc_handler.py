import streamlit as st
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from utils.build_graph import build_knowledge_graph
from rank_bm25 import BM25Okapi
import os
import re
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
import nltk


try:
    nltk.download('punkt', quiet=True)
except:
    pass

def preprocess_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s.,!?;:\-\'\"()]', '', text)
    return text.strip()


def process_document_batch(file_batch):
    documents = []
    for file in file_batch:
        try:
            st.info(f"Äang xá»­ lÃ½ file: {file.name}")
            file_path = os.path.join("temp", file.name)
            with open(file_path, "wb") as f:
                f.write(file.getbuffer())

            if file.name.endswith(".pdf"):
                st.write("Äang Ä‘á»c file PDF...")
                loader = PyPDFLoader(file_path)
            elif file.name.endswith(".docx"):
                st.write("Äang Ä‘á»c file DOCX...")
                loader = Docx2txtLoader(file_path)
            elif file.name.endswith(".txt"):
                st.write("Äang Ä‘á»c file TXT...")
                loader = TextLoader(file_path)
            else:
                st.warning(f"Äá»‹nh dáº¡ng file khÃ´ng Ä‘Æ°á»£c há»— trá»£: {file.name}")
                continue

            docs = loader.load()
            if not docs:
                st.warning(f"KhÃ´ng tÃ¬m tháº¥y ná»™i dung trong file: {file.name}")
                continue

            st.success(f"ÄÃ£ Ä‘á»c Ä‘Æ°á»£c {len(docs)} trang/pháº§n tá»« {file.name}")

            for doc in docs:
                doc.page_content = preprocess_text(doc.page_content)
                if not doc.page_content.strip():
                    st.warning(f"Trang/pháº§n trong {file.name} khÃ´ng cÃ³ ná»™i dung sau khi xá»­ lÃ½")
                    continue
                doc.metadata['source'] = file.name
                documents.append(doc)

            os.remove(file_path)
            st.success(f"ÄÃ£ xá»­ lÃ½ xong file: {file.name}")
        except Exception as e:
            st.error(f"Lá»—i xá»­ lÃ½ file {file.name}: {str(e)}")

    st.info(f"Tá»•ng sá»‘ documents Ä‘Ã£ xá»­ lÃ½ trong batch: {len(documents)}")
    return documents

def process_documents(uploaded_files, reranker, openai_api_key):
    if st.session_state.documents_loaded:
        return

    st.session_state.processing = True

    if not uploaded_files:
        st.error("KhÃ´ng cÃ³ file nÃ o Ä‘Æ°á»£c táº£i lÃªn!")
        return

    st.info(f"Báº¯t Ä‘áº§u xá»­ lÃ½ {len(uploaded_files)} files...")

    if not os.path.exists("temp"):
        os.makedirs("temp")

    batch_size = 4
    with ThreadPoolExecutor(max_workers=batch_size) as executor:
        file_batches = [uploaded_files[i:i + batch_size] for i in range(0, len(uploaded_files), batch_size)]
        documents = []
        with st.spinner('Äang xá»­ lÃ½ documents...'):
            for batch_docs in tqdm(executor.map(process_document_batch, file_batches), total=len(file_batches)):
                documents.extend(batch_docs)

    st.info(f"Sá»‘ lÆ°á»£ng documents sau khi xá»­ lÃ½: {len(documents)}")

    if not documents:
        st.error("KhÃ´ng cÃ³ documents nÃ o Ä‘Æ°á»£c xá»­ lÃ½ thÃ nh cÃ´ng!")
        return

    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )

    texts = text_splitter.split_documents(documents)

    for i, text in enumerate(texts):
        text.metadata['chunk_id'] = i
        text.metadata['total_chunks'] = len(texts)

    text_contents = [doc.page_content for doc in texts]

    if not texts:
        st.error("KhÃ´ng tÃ¬m tháº¥y vÄƒn báº£n há»£p lá»‡ sau khi xá»­ lÃ½ documents!")
        return

    st.info(f"Äang xá»­ lÃ½ {len(texts)} Ä‘oáº¡n vÄƒn báº£n...")

    if not any(text.page_content.strip() for text in texts):
        st.error("KhÃ´ng cÃ³ ná»™i dung há»£p lá»‡ trong cÃ¡c documents!")
        return

    try:
        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
        test_embedding = embeddings.embed_query("test")
        if not test_embedding:
            st.error("Failed to generate embeddings! Please check your OpenAI API key.")
            return
    except Exception as e:
        st.error(f"Error with OpenAI embeddings: {str(e)}")
        return

    try:
        vector_store = Chroma.from_documents(
            texts,
            embedding=embeddings,
            persist_directory="./chroma_store"
        )
        vector_store.persist()
    except Exception as e:
        st.error(f"Error creating vector store: {str(e)}")
        return

    bm25_retriever = BM25Retriever.from_texts(
        text_contents,
        bm25_impl=BM25Okapi,
        preprocess_func=lambda text: re.sub(r"\W+", " ", text).lower().split()
    )

    ensemble_retriever = EnsembleRetriever(
        retrievers=[
            bm25_retriever,
            vector_store.as_retriever(search_kwargs={"k": 5})
        ],
        weights=[0.4, 0.6]
    )

    st.session_state.retrieval_pipeline = {
        "ensemble": ensemble_retriever,
        "reranker": reranker,
        "texts": text_contents,
        "knowledge_graph": build_knowledge_graph(texts)
    }

    st.success(f"âœ… ÄÃ£ xá»­ lÃ½ thÃ nh cÃ´ng {len(uploaded_files)} files!")
    st.write(f"ğŸ“Š Tá»•ng sá»‘ chunks: {len(texts)}")
    st.write(f"ğŸ“Š KÃ­ch thÆ°á»›c trung bÃ¬nh má»—i chunk: {sum(len(t.page_content) for t in texts)/len(texts):.0f} kÃ½ tá»±")

    st.session_state.documents_loaded = True
    st.session_state.processing = False

    if "knowledge_graph" in st.session_state.retrieval_pipeline:
        G = st.session_state.retrieval_pipeline["knowledge_graph"]
        st.write(f"ğŸ”— Total Nodes: {len(G.nodes)}")
        st.write(f"ğŸ”— Total Edges: {len(G.edges)}")
        st.write(f"ğŸ”— Sample Nodes: {list(G.nodes)[:10]}")
        st.write(f"ğŸ”— Sample Edges: {list(G.edges)[:10]}")
